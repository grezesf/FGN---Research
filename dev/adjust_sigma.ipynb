{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev of adjust_sigma.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/felix/Research/Adversarial Research/FGN---Research/')\n",
    "import Finite_Gaussian_Network_lib as fgnl\n",
    "import Finite_Gaussian_Network_lib.fgn_helper_lib as fgnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset and dataloader declaration\n",
    "# transforms does both the conversion from 0-255 to 0-1\n",
    "# and normalizes by the precomputed mean and std\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "mnist_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../MNIST-dataset', train=True, download=False, \n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])), \n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sigma(fgn_model, dataloader=None, loss_func=None, pred_func=None, verbose=False):\n",
    "    \n",
    "    ###\n",
    "    # will attempt to adjust the sigmas in the FGN model\n",
    "    # if no data is given, will adjust based on mean 0 var 1 input to get as close to mean 0 var 1 output\n",
    "    # if data is given, will adjust based on it to get as close to mean 0 var 1 output\n",
    "    # if data+loss function given, will minimize loss\n",
    "    # if data+pred_func given, will maximize accuracy\n",
    "    ###\n",
    "    \n",
    "    ### type checks\n",
    "    # model: a Pytorch module\n",
    "    if not isinstance(model, fgnl.Feedforward_FGN_net):\n",
    "        raise TypeError(\"model is not pytorch  FGN module\")\n",
    "    # dataset: a pytorch data loader\n",
    "    if (dataloader is not None) and (not isinstance(dataloader, torch.utils.data.dataloader.DataLoader)):\n",
    "        raise TypeError(\"test_loader is not pytorch dataloader\")\n",
    "    # loss_func: a pytorch loss function (can be any function)\n",
    "    if (loss_func is not None) and (not callable(loss_func)):\n",
    "        raise TypeError(\"loss_func is not a function\")\n",
    "    # loss_func: a pred function (can be any function)\n",
    "    if (pred_func is not None) and (not callable(pred_func)):\n",
    "        raise TypeError(\"pred_func is not a function\")\n",
    "    # verbose: bool\n",
    "    if not isinstance(verbose, bool):\n",
    "        raise TypeError(\"verbose is not a boolean\")\n",
    "    \n",
    "    # if the dataloader is empty, adjust sigmas to get mean=0 variance=0\n",
    "    if dataloader==None:\n",
    "        pass\n",
    "        \n",
    "    # if a dataset (and either loss function or pred_func) is provided, adjust to fit the data best\n",
    "#     if :\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "    # return nothing\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sigma_pred_func(fgn_model, dataloader, pred_func, verbose):\n",
    "    \n",
    "    ###\n",
    "    # adjusts the sigmas of the given fgn model so that the pred accuracy over the dataset is max\n",
    "    ###\n",
    "    \n",
    "    # best pred acc yet\n",
    "    fgn_test_res = fgnh.test(fgn_model, dataloader, \n",
    "                             (lambda model, output, target:torch.tensor(0)), verbose=verbose, \n",
    "                             pred_func=pred_func)\n",
    "    best_pred = fgn_test_res['test_accuracy']\n",
    "    # best sigma multiplier yet\n",
    "    best_sig_mult = 1.0\n",
    "    # lower bound for sigma mult\n",
    "    lower_bound = 0.0\n",
    "    # uper bound for sigma\n",
    "    upper_bound = float('Inf')\n",
    "    \n",
    "    # max number of values to test\n",
    "    max_iter = 25\n",
    "    \n",
    "    # first double sigmas until performance decreases\n",
    "    for ite in range(max_iter):\n",
    "        # new val to test\n",
    "        cur_sig_mult = 2.0*best_sig_mult\n",
    "        if verbose: print(ite, \"testing\", cur_sig_mult)\n",
    "        \n",
    "        # apply multiplier\n",
    "        # given an fgn model, multiplies all the sigmas by a value\n",
    "        for p in fgn_model.modules():\n",
    "            if isinstance(p, fgnl.FGN_layer):\n",
    "                p.sigmas = torch.nn.Parameter(p.sigmas*cur_sig_mult)\n",
    "    \n",
    "        # test\n",
    "        fgn_test_res = fgnh.test(fgn_model, dataloader, \n",
    "                             (lambda model, output, target:torch.tensor(0)), verbose=verbose, \n",
    "                             pred_func=pred_func)\n",
    "        cur_pred = fgn_test_res['test_accuracy']\n",
    "        \n",
    "        # reset sigmas \n",
    "        for p in fgn_model.modules():\n",
    "            if isinstance(p, fgnl.FGN_layer):\n",
    "                p.sigmas = torch.nn.Parameter(p.sigmas/cur_sig_mult)\n",
    "                \n",
    "        if cur_pred > best_pred:\n",
    "            if verbose: print(\"new best during doubling\")\n",
    "            # new best\n",
    "            best_pred = cur_pred\n",
    "            best_sig_mult = cur_sig_mult\n",
    "            # increase lower bound\n",
    "            lower_bound = cur_sig_mult\n",
    "        else:\n",
    "            # new upper bound\n",
    "            upper_bound = cur_sig_mult\n",
    "            # and exit loop\n",
    "            break\n",
    "            \n",
    "            \n",
    "    # next half sigmas until performance decreases\n",
    "    for ite in range(max_iter):\n",
    "        # new val to test\n",
    "        cur_sig_mult = 0.5*best_sig_mult\n",
    "        if verbose: print(ite, \"testing\", cur_sig_mult)\n",
    "        \n",
    "        # apply multiplier\n",
    "        # given an fgn model, multiplies all the sigmas by a value\n",
    "        for p in fgn_model.modules():\n",
    "            if isinstance(p, fgnl.FGN_layer):\n",
    "                p.sigmas = torch.nn.Parameter(p.sigmas*cur_sig_mult)\n",
    "    \n",
    "        # test\n",
    "        fgn_test_res = fgnh.test(fgn_model, dataloader, \n",
    "                             (lambda model, output, target:torch.tensor(0)), verbose=verbose, \n",
    "                             pred_func=pred_func)\n",
    "        cur_pred = fgn_test_res['test_accuracy']\n",
    "        \n",
    "        # reset sigmas \n",
    "        for p in fgn_model.modules():\n",
    "            if isinstance(p, fgnl.FGN_layer):\n",
    "                p.sigmas = torch.nn.Parameter(p.sigmas/cur_sig_mult)\n",
    "                \n",
    "        if cur_pred >= (1.0-1e-3)*best_pred:\n",
    "            if verbose: print(\"new best during halfing\")\n",
    "            # new best\n",
    "            best_pred = cur_pred\n",
    "            best_sig_mult = cur_sig_mult\n",
    "            # new upper bound\n",
    "            upper_bound = cur_sig_mult\n",
    "        else:\n",
    "            # increase lower bound\n",
    "            lower_bound = cur_sig_mult\n",
    "            # and exit loop\n",
    "            break\n",
    "    \n",
    "    # now that we have a real bounds, search by dichotomie\n",
    "    for ite in range(max_iter):\n",
    "\n",
    "        # new val to test\n",
    "        cur_sig_mult = 0.5*(upper_bound+lower_bound)\n",
    "        if verbose: print(ite, \"testing\", cur_sig_mult)\n",
    "        \n",
    "        # apply multiplier\n",
    "        # given an fgn model, multiplies all the sigmas by a value\n",
    "        for p in fgn_model.modules():\n",
    "            if isinstance(p, fgnl.FGN_layer):\n",
    "                p.sigmas = torch.nn.Parameter(p.sigmas*cur_sig_mult)\n",
    "    \n",
    "        # test\n",
    "        fgn_test_res = fgnh.test(fgn_model, dataloader, \n",
    "                             (lambda model, output, target:torch.tensor(0)), verbose=verbose, \n",
    "                             pred_func=pred_func)\n",
    "        cur_pred = fgn_test_res['test_accuracy']\n",
    "        \n",
    "        # reset sigmas \n",
    "        for p in fgn_model.modules():\n",
    "            if isinstance(p, fgnl.FGN_layer):\n",
    "                p.sigmas = torch.nn.Parameter(p.sigmas/cur_sig_mult)\n",
    "                \n",
    "        if cur_pred >= (1.0-1e-3)*best_pred:\n",
    "            if verbose: print(\"new best during dicho\")\n",
    "            # new low bound\n",
    "            if cur_sig_mult > best_sig_mult:\n",
    "                lower_bound = cur_sig_mult\n",
    "            # new upper bound\n",
    "            else:\n",
    "                upper_bound = cur_sig_mult\n",
    "            # new best\n",
    "            best_pred = cur_pred\n",
    "            best_sig_mult = cur_sig_mult\n",
    "                \n",
    "        else:\n",
    "            # new low bound\n",
    "            if cur_sig_mult < best_sig_mult:\n",
    "                lower_bound = cur_sig_mult\n",
    "            # new upper bound\n",
    "            else:\n",
    "                upper_bound = cur_sig_mult\n",
    "            \n",
    "    # apply best mult\n",
    "    if verbose: print(\"best multiplier:\", best_sig_mult)\n",
    "    for p in fgn_model.modules():\n",
    "        if isinstance(p, fgnl.FGN_layer):\n",
    "            p.sigmas = torch.nn.Parameter(p.sigmas*best_sig_mult)\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model to test:\n",
    "# Initialize the classic network\n",
    "hidden_l_nums = [16,16,16]\n",
    "drop_p= 0.0\n",
    "fgn_model = fgnl.Feedforward_FGN_net(in_feats=28*28, out_feats=10, hidden_l_nums=hidden_l_nums, drop_p=drop_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.66666666667e-06\n",
      "6.73333333333e-06\n",
      "Epoch 0 Train set - Average loss: 1.0402, Accuracy: 41953/60000 (70%)\n",
      "Epoch 1 Train set - Average loss: 0.3339, Accuracy: 54701/60000 (91%)\n",
      "Epoch 2 Train set - Average loss: 0.2543, Accuracy: 55954/60000 (93%)\n"
     ]
    }
   ],
   "source": [
    "# train it\n",
    "# loss functions for the classic net\n",
    "lmbda_l2 = (4.0*0.1/len(mnist_train_loader.dataset))\n",
    "print(lmbda_l2)\n",
    "lmbda_sigs = 1.01*lmbda_l2\n",
    "print(lmbda_sigs)\n",
    "loss_func = fgnl.def_fgn_cross_ent_loss(lmbda_l2,lmbda_sigs)\n",
    "\n",
    "fgn_optimizer = optim.RMSprop(filter(lambda p: p.requires_grad, fgn_model.parameters()),momentum=0.5)\n",
    "\n",
    "epochs=3\n",
    "\n",
    "fgn_train_res = fgnh.train(fgn_model, mnist_train_loader,\n",
    "                           loss_func, fgn_optimizer, epochs, \n",
    "                           pred_func=fgnh.cross_ent_pred_accuracy, save_hist=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set - Average loss: 0.3423, Accuracy: 54457/60000 (91%)\n"
     ]
    }
   ],
   "source": [
    "# make sure performance before isnt crap\n",
    "fgn_test_res_pre = fgnh.test(fgn_model, mnist_train_loader, \n",
    "                     loss_func, verbose=True, \n",
    "                     pred_func=fgnh.cross_ent_pred_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set - Average loss: 0.0000, Accuracy: 54457/60000 (91%)\n",
      "0 testing 2.0\n",
      "Test set - Average loss: 0.0000, Accuracy: 5959/60000 (10%)\n",
      "0 testing 0.5\n",
      "Test set - Average loss: 0.0000, Accuracy: 5851/60000 (10%)\n",
      "0 testing 1.25\n",
      "Test set - Average loss: 0.0000, Accuracy: 5933/60000 (10%)\n",
      "1 testing 0.875\n",
      "Test set - Average loss: 0.0000, Accuracy: 5958/60000 (10%)\n",
      "2 testing 1.0625\n",
      "Test set - Average loss: 0.0000, Accuracy: 9350/60000 (16%)\n",
      "3 testing 0.96875\n",
      "Test set - Average loss: 0.0000, Accuracy: 39387/60000 (66%)\n",
      "4 testing 1.015625\n",
      "Test set - Average loss: 0.0000, Accuracy: 49501/60000 (83%)\n",
      "5 testing 0.9921875\n",
      "Test set - Average loss: 0.0000, Accuracy: 53030/60000 (88%)\n",
      "6 testing 1.00390625\n",
      "Test set - Average loss: 0.0000, Accuracy: 54407/60000 (91%)\n",
      "new best during dicho\n",
      "7 testing 1.009765625\n",
      "Test set - Average loss: 0.0000, Accuracy: 53109/60000 (89%)\n",
      "8 testing 1.0068359375\n",
      "Test set - Average loss: 0.0000, Accuracy: 53970/60000 (90%)\n",
      "9 testing 1.00537109375\n",
      "Test set - Average loss: 0.0000, Accuracy: 54240/60000 (90%)\n",
      "10 testing 1.00463867188\n",
      "Test set - Average loss: 0.0000, Accuracy: 54356/60000 (91%)\n",
      "new best during dicho\n",
      "11 testing 1.00500488281\n",
      "Test set - Average loss: 0.0000, Accuracy: 54286/60000 (90%)\n",
      "12 testing 1.00482177734\n",
      "Test set - Average loss: 0.0000, Accuracy: 54326/60000 (91%)\n",
      "new best during dicho\n",
      "13 testing 1.00491333008\n",
      "Test set - Average loss: 0.0000, Accuracy: 54315/60000 (91%)\n",
      "new best during dicho\n",
      "14 testing 1.00495910645\n",
      "Test set - Average loss: 0.0000, Accuracy: 54299/60000 (90%)\n",
      "new best during dicho\n",
      "15 testing 1.00498199463\n",
      "Test set - Average loss: 0.0000, Accuracy: 54290/60000 (90%)\n",
      "new best during dicho\n",
      "16 testing 1.00499343872\n",
      "Test set - Average loss: 0.0000, Accuracy: 54288/60000 (90%)\n",
      "new best during dicho\n",
      "17 testing 1.00499916077\n",
      "Test set - Average loss: 0.0000, Accuracy: 54287/60000 (90%)\n",
      "new best during dicho\n",
      "18 testing 1.00500202179\n",
      "Test set - Average loss: 0.0000, Accuracy: 54285/60000 (90%)\n",
      "new best during dicho\n",
      "19 testing 1.0050034523\n",
      "Test set - Average loss: 0.0000, Accuracy: 54286/60000 (90%)\n",
      "new best during dicho\n",
      "20 testing 1.00500416756\n",
      "Test set - Average loss: 0.0000, Accuracy: 54286/60000 (90%)\n",
      "new best during dicho\n",
      "21 testing 1.00500452518\n",
      "Test set - Average loss: 0.0000, Accuracy: 54286/60000 (90%)\n",
      "new best during dicho\n",
      "22 testing 1.005004704\n",
      "Test set - Average loss: 0.0000, Accuracy: 54286/60000 (90%)\n",
      "new best during dicho\n",
      "23 testing 1.00500479341\n",
      "Test set - Average loss: 0.0000, Accuracy: 54286/60000 (90%)\n",
      "new best during dicho\n",
      "24 testing 1.00500483811\n",
      "Test set - Average loss: 0.0000, Accuracy: 54286/60000 (90%)\n",
      "new best during dicho\n",
      "best multiplier: 1.00500483811\n"
     ]
    }
   ],
   "source": [
    "adjust_sigma_pred_func(fgn_model, mnist_train_loader, fgnh.cross_ent_pred_accuracy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set - Average loss: 0.3577, Accuracy: 54286/60000 (90%)\n"
     ]
    }
   ],
   "source": [
    "# make sure performance after isnt crap\n",
    "fgn_test_res_post = fgnh.test(fgn_model, mnist_train_loader, \n",
    "                     loss_func, verbose=True, \n",
    "                     pred_func=fgnh.cross_ent_pred_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
