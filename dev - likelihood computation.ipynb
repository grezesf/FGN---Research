{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev code to compute likelihood of the data over the gaussians of a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# set this 'backend' when using jupyter; do this before importing pyplot\n",
    "mpl.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# sys.path.append('/home/felix/Research/Adversarial Research/FGN---Research')\n",
    "import torch_helper_lib as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "# random seeds\n",
    "torch.manual_seed(665)\n",
    "np.random.seed(3326)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(999)\n",
    "\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define nd Toy Data\n",
    "\n",
    "# number of dimensions of the data\n",
    "num_dim = 2\n",
    "\n",
    "# gaussian target parameters\n",
    "data_centers = 1*np.ones(num_dim)\n",
    "sigma = 1\n",
    "\n",
    "# hyper-plane separating the classes (this will become the target for the weights)\n",
    "sep_plane = np.random.uniform(low=-1.0, high=1.0, size=num_dim)\n",
    "# sep_plane = np.concatenate(([1],np.zeros(num_dim-1)))\n",
    "\n",
    "num_samples = 500\n",
    "\n",
    "samples_xs = np.array([np.random.normal(loc=0, scale=sigma, size=num_dim)+data_centers for _ in range(num_samples)] )\n",
    "\n",
    "# apply labels based on side of sep hyper plane\n",
    "samples_labels = np.array([ [1] if x>np.matmul(data_centers, sep_plane) else [-1] for x in np.matmul(samples_xs, sep_plane)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to pytorch format \n",
    "tensor_x = torch.Tensor(samples_xs)\n",
    "tensor_y = torch.Tensor(samples_labels)\n",
    "\n",
    "my_dataset = torch.utils.data.TensorDataset(tensor_x[:num_samples*4/5],tensor_y[:num_samples*4/5]) # create your dataset\n",
    "my_test_data = torch.utils.data.TensorDataset(tensor_x[num_samples*4/5:],tensor_y[num_samples*4/5:]) # create your dataset\n",
    "\n",
    "my_dataloader = torch.utils.data.DataLoader(my_dataset, batch_size=5) # create your dataloader\n",
    "my_test_dataloader = torch.utils.data.DataLoader(my_test_data) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 2])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the FGN layer class to dev\n",
    "\n",
    "class FGN_layer(nn.Module):\n",
    "    r\"\"\" Applies a Finite Gaussian Neuron layer to the incoming data\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    Shape:\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "    Examples:\n",
    "        \n",
    "        >>> l=FGN_layer(20,30)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(FGN_layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # learnable parameters\n",
    "        # regular NN weights (transposed at the start, see order of Tensor(dims))\n",
    "        self.weights = nn.Parameter(torch.Tensor(out_features, in_features), requires_grad = True)\n",
    "        # centers of FGNs\n",
    "        self.centers = nn.Parameter(torch.Tensor(out_features, in_features), requires_grad = True)\n",
    "        # size of FGNs\n",
    "        self.sigs = nn.Parameter(torch.Tensor(out_features,), requires_grad = True)\n",
    "        # importance of each gaussian for likelihoods\n",
    "        self.pis = nn.Parameter(torch.Tensor(out_features,), requires_grad = True)\n",
    "        # epsilon\n",
    "        self.eps = 1e-7        \n",
    "        \n",
    "        # parameter init call\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    # parameter init definition\n",
    "    def reset_parameters(self):\n",
    "        s = np.sqrt(self.in_features)\n",
    "        # regular NN init\n",
    "        self.weights.data.uniform_(-s, s)\n",
    "        # centers init, assuming data normalized to mean 0 var 1\n",
    "        self.centers.data.uniform_(-0.01, 0.01)\n",
    "        # size init, to be researched further\n",
    "        self.sigs.data.uniform_(0.99*self.in_features, 1.01*self.in_features)\n",
    "        # PIs init, start at 1/n each\n",
    "        self.pis.data.fill_(1.0/self.out_features)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        # linear part is the same as normal NNs\n",
    "        biases = -torch.sum(torch.mul(self.weights, self.centers), dim=-1)\n",
    "        l = F.linear(input, self.weights, bias=biases)\n",
    "        # optional, apply tanh here\n",
    "        # l = torch.tanh(l)\n",
    "\n",
    "        # gaussian component\n",
    "        # unsqueeze the inputs to allow broadcasting\n",
    "        # compute distance to centers\n",
    "        g = (input.unsqueeze(1)-self.centers)**2\n",
    "        g = g.sum(dim=2)\n",
    "\n",
    "        # for future, use any norm?\n",
    "#         g = torch.norm(input.unsqueeze(1)-self.centers, p=1, dim=2)\n",
    "\n",
    "        # apply sigma\n",
    "        g = -g/(self.sigs**2)\n",
    "        # apply exponential\n",
    "        g = torch.exp(g)\n",
    "\n",
    "        # combine gaussian with linear\n",
    "        res = l*g\n",
    "        # optional, flatten res\n",
    "        # res = F.tanh(res)\n",
    "\n",
    "        # likelihoods computation for each data point\n",
    "        if self.training:\n",
    "            likelihoods = input.unsqueeze(1)\n",
    "            likelihoods = likelihoods - self.centers\n",
    "            likelihoods = likelihoods**2\n",
    "            likelihoods = torch.sum(likelihoods, dim=-1)\n",
    "            likelihoods = likelihoods/(self.sigs**2)\n",
    "            # add ln(det(SIG)) = 2k*log(sig)\n",
    "            likelihoods = likelihoods + 2*self.in_features*torch.log(self.sigs)\n",
    "            # at this stage, all are ~ -ln N(sample|gaussian) for each gaussian in layer\n",
    "            #multiply by the PIs, constrained to sum to 1\n",
    "            pis_normalized = F.softmax(self.pis, dim=-1)\n",
    "            likelihoods = likelihoods*pis_normalized\n",
    "            # sum them up\n",
    "            likelihoods = torch.sum(likelihoods, dim=-1)\n",
    "\n",
    "        else:\n",
    "            likelihoods = torch.tensor([-1.0])\n",
    "        \n",
    "        return res, likelihoods\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Finite Gaussian Neural Network\n",
    "class FGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FGNet, self).__init__()\n",
    "        self.l1 = FGN_layer(num_dim,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, l = self.l1(x)\n",
    "        # clip res to +1\n",
    "        x  = torch.clamp(x, min=-1.0, max=1.0)\n",
    "\n",
    "        return x,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "fgn_model = FGNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood only loss function\n",
    "def loss_func(model, output, target):\n",
    "    # get output and likelihoods\n",
    "    pred, likelihoods = output\n",
    "    return torch.sum(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.SGD(fgn_model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch number\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('l1.weights', tensor([[-1.0900,  0.6888],\n",
      "        [-0.7106,  0.4763],\n",
      "        [-0.4106,  1.0383]], device='cuda:0')), ('l1.centers', tensor([[ 0.0052,  0.0076],\n",
      "        [-0.0089,  0.0033],\n",
      "        [-0.0095, -0.0062]], device='cuda:0')), ('l1.sigs', tensor([1.9921, 1.9843, 2.0009], device='cuda:0')), ('l1.pis', tensor([0.3333, 0.3333, 0.3333], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# model state dict before training\n",
    "print(fgn_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train set - Average loss: 12.5958\n",
      "Epoch 1 Train set - Average loss: 10.1052\n",
      "Epoch 2 Train set - Average loss: 10.1053\n",
      "Epoch 3 Train set - Average loss: 10.1053\n",
      "Epoch 4 Train set - Average loss: 10.1053\n"
     ]
    }
   ],
   "source": [
    "### train:\n",
    "res1 = th.train(fgn_model, device, my_dataloader, loss_func, optimizer, epochs, save_hist=2, verbose=True, pred_func=None, test_loader=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('l1.weights', tensor([[-1.0900,  0.6888],\n",
      "        [-0.7106,  0.4763],\n",
      "        [-0.4106,  1.0383]], device='cuda:0')), ('l1.centers', tensor([[1.0217, 1.1669],\n",
      "        [1.0220, 1.1661],\n",
      "        [1.0227, 1.1639]], device='cuda:0')), ('l1.sigs', tensor([0.9092, 0.9096, 0.9109], device='cuda:0')), ('l1.pis', tensor([0.3458, 0.3383, 0.3159], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# model state dict after training\n",
    "print(fgn_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set - Average loss: -1.0000\n"
     ]
    }
   ],
   "source": [
    "### test \n",
    "res2 = th.test(fgn_model, device, my_test_dataloader, loss_func, verbose=True, pred_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BELOW == DEV work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood for a batch\n",
    "batch_x, batch_y = next(iter(my_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(batch_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0389, -1.9596], device='cuda:0')\n",
      "tensor([1.0217, 1.1669], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.9092, device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# for one sample\n",
    "# for one gaussian\n",
    "sample = batch_x[0].to(device)\n",
    "gaussian_centers = fgn_model.l1.centers[0]\n",
    "sig = fgn_model.l1.sigs[0]\n",
    "print(sample)\n",
    "print(gaussian_centers)\n",
    "print(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([ 0.0172, -3.1264], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "1 tensor([2.9491e-04, 9.7747e+00], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "2 tensor(9.7750, device='cuda:0', grad_fn=<SumBackward2>)\n",
      "3 tensor(11.8247, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "4 tensor(11.4439, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# log likelihood based on https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Likelihood_function\n",
    "\n",
    "ite =  iter(xrange(99))\n",
    "\n",
    "# sample minus centers\n",
    "x = sample-gaussian_centers\n",
    "print(next(ite), x)\n",
    "# X.T * SIG-1 * X\n",
    "x = x**2\n",
    "print(next(ite), x)\n",
    "x = torch.sum(x, dim=-1)\n",
    "print(next(ite), x)\n",
    "x = x/(sig**2)\n",
    "print(next(ite), x)\n",
    "# add ln(det(SIG)) = 2k*log(sig)\n",
    "x = x + 2*batch_x.shape[-1]*torch.log(sig)\n",
    "print(next(ite), x)\n",
    "# x is now the negative log likelihood of this sample for this gaussian\n",
    "\n",
    "# without the constants, which we ignore (correct?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[ 0.0172, -3.1264],\n",
      "        [ 0.0169, -3.1257],\n",
      "        [ 0.0162, -3.1235]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "1 tensor([[2.9491e-04, 9.7747e+00],\n",
      "        [2.8670e-04, 9.7700e+00],\n",
      "        [2.6182e-04, 9.7561e+00]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "2 tensor([9.7750, 9.7703, 9.7564], device='cuda:0', grad_fn=<SumBackward2>)\n",
      "3 tensor([11.8247, 11.8082, 11.7590], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "4 tensor([11.4439, 11.4293, 11.3856], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5 tensor([3.9571, 3.8667, 3.5968], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "6 tensor(11.4206, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ite =  iter(xrange(99))\n",
    "\n",
    "# now more general case, mix of gaussian, still one sample\n",
    "x = sample - fgn_model.l1.centers\n",
    "print(next(ite), x)\n",
    "x = x**2\n",
    "print(next(ite), x)\n",
    "x = torch.sum(x, dim=-1)\n",
    "print(next(ite), x)\n",
    "x = x/(fgn_model.l1.sigs**2)\n",
    "print(next(ite), x)\n",
    "x = x + 2*batch_x.shape[-1]*torch.log(fgn_model.l1.sigs)\n",
    "print(next(ite), x)\n",
    "# at this stage, all are ~ -ln N(sample|gaussian) for each gaussian in layer\n",
    "\n",
    "#multiply by the PIs\n",
    "x = x*fgn_model.l1.pis\n",
    "print(next(ite), x)\n",
    "# sum them up\n",
    "x = torch.sum(x)\n",
    "print(next(ite), x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[[ 1.0389, -1.9596]],\n",
      "\n",
      "        [[ 2.6153,  0.7124]],\n",
      "\n",
      "        [[ 0.9541, -0.2875]],\n",
      "\n",
      "        [[ 1.2648,  1.1148]],\n",
      "\n",
      "        [[-0.4840,  2.7405]]], device='cuda:0')\n",
      "1 tensor([[[ 0.0172, -3.1264],\n",
      "         [ 0.0169, -3.1257],\n",
      "         [ 0.0162, -3.1235]],\n",
      "\n",
      "        [[ 1.5936, -0.4545],\n",
      "         [ 1.5933, -0.4537],\n",
      "         [ 1.5926, -0.4515]],\n",
      "\n",
      "        [[-0.0677, -1.4544],\n",
      "         [-0.0679, -1.4536],\n",
      "         [-0.0686, -1.4514]],\n",
      "\n",
      "        [[ 0.2431, -0.0521],\n",
      "         [ 0.2429, -0.0514],\n",
      "         [ 0.2421, -0.0491]],\n",
      "\n",
      "        [[-1.5057,  1.5736],\n",
      "         [-1.5060,  1.5744],\n",
      "         [-1.5067,  1.5766]]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "2 tensor([[[2.9491e-04, 9.7747e+00],\n",
      "         [2.8670e-04, 9.7700e+00],\n",
      "         [2.6182e-04, 9.7561e+00]],\n",
      "\n",
      "        [[2.5394e+00, 2.0656e-01],\n",
      "         [2.5386e+00, 2.0589e-01],\n",
      "         [2.5362e+00, 2.0388e-01]],\n",
      "\n",
      "        [[4.5768e-03, 2.1151e+00],\n",
      "         [4.6094e-03, 2.1130e+00],\n",
      "         [4.7120e-03, 2.1065e+00]],\n",
      "\n",
      "        [[5.9100e-02, 2.7149e-03],\n",
      "         [5.8983e-02, 2.6378e-03],\n",
      "         [5.8618e-02, 2.4146e-03]],\n",
      "\n",
      "        [[2.2672e+00, 2.4763e+00],\n",
      "         [2.2680e+00, 2.4786e+00],\n",
      "         [2.2702e+00, 2.4856e+00]]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "3 tensor([[9.7750, 9.7703, 9.7564],\n",
      "        [2.7460, 2.7445, 2.7401],\n",
      "        [2.1197, 2.1176, 2.1112],\n",
      "        [0.0618, 0.0616, 0.0610],\n",
      "        [4.7435, 4.7466, 4.7559]], device='cuda:0', grad_fn=<SumBackward2>)\n",
      "4 tensor([[11.8247, 11.8082, 11.7590],\n",
      "        [ 3.3218,  3.3170,  3.3026],\n",
      "        [ 2.5642,  2.5593,  2.5446],\n",
      "        [ 0.0748,  0.0745,  0.0736],\n",
      "        [ 5.7382,  5.7367,  5.7321]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "5 tensor([[11.4439, 11.4293, 11.3856],\n",
      "        [ 2.9410,  2.9381,  2.9292],\n",
      "        [ 2.1835,  2.1804,  2.1712],\n",
      "        [-0.3060, -0.3044, -0.2998],\n",
      "        [ 5.3575,  5.3578,  5.3587]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "6 tensor([[ 3.9571,  3.8667,  3.5968],\n",
      "        [ 1.0170,  0.9940,  0.9253],\n",
      "        [ 0.7550,  0.7376,  0.6859],\n",
      "        [-0.1058, -0.1030, -0.0947],\n",
      "        [ 1.8525,  1.8126,  1.6928]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "7 tensor([11.4206,  2.9363,  2.1785, -0.3035,  5.3580], device='cuda:0',\n",
      "       grad_fn=<SumBackward2>)\n"
     ]
    }
   ],
   "source": [
    "ite =  iter(xrange(99))\n",
    "\n",
    "# now for a whole batch, for all gaussians in a layer\n",
    "x = batch_x.unsqueeze(1).to(device)\n",
    "print(next(ite), x)\n",
    "x = x - fgn_model.l1.centers\n",
    "print(next(ite), x)\n",
    "x = x**2\n",
    "print(next(ite), x)\n",
    "x = torch.sum(x, dim=-1)\n",
    "print(next(ite), x)\n",
    "x = x/(fgn_model.l1.sigs**2)\n",
    "print(next(ite), x)\n",
    "# add ln(det(SIG)) = 2k*log(sig)\n",
    "x = x + 2*batch_x.shape[-1]*torch.log(fgn_model.l1.sigs)\n",
    "print(next(ite), x)\n",
    "# at this stage, all are ~ -ln N(sample|gaussian) for each gaussian in layer\n",
    "#multiply by the PIs\n",
    "x = x*fgn_model.l1.pis\n",
    "print(next(ite), x)\n",
    "# sum them up\n",
    "x = torch.sum(x, dim=-1)\n",
    "print(next(ite), x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4743bf39306a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# constrain PIs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpis\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfgn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# constrain PIs\n",
    "print(fgn_model.l1.pis / fgn_model.l1.pis.sum(1, keepdim=True).clamp(min=1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
