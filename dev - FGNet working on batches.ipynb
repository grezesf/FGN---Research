{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev code for making FGNets works on batches of data instead of a single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# graph viz tool\n",
    "# import sys\n",
    "# sys.path.append('/home/felix/Research/Adversarial Research/FGN---Research')\n",
    "import torch_helper_lib as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the FGN layer class\n",
    "import math \n",
    "\n",
    "class FGN_layer(nn.Module):\n",
    "    r\"\"\" Applies a Finite Gaussian Neuron layer to the incoming data\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    Shape:\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "    Examples:\n",
    "        \n",
    "        >>> l=FGN_layer(20,30)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(FGN_layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # learnable parameters\n",
    "        # regular NN weights (transposed at the start, see order of Tensor(dims))\n",
    "        self.weights = nn.Parameter(torch.Tensor(out_features, in_features), requires_grad= True)\n",
    "        # centers of FGNs\n",
    "        self.centers = nn.Parameter(torch.Tensor(out_features, in_features), requires_grad = True)\n",
    "        # size of FGNs\n",
    "        self.sigs = nn.Parameter(torch.Tensor(out_features,), requires_grad = True)\n",
    "        \n",
    "        # parameter init call\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    # parameter init definition\n",
    "    def reset_parameters(self):\n",
    "        s = np.sqrt(self.in_features)\n",
    "        # regular NN init\n",
    "        self.weights.data.uniform_(-s, s)\n",
    "        # centers init\n",
    "        self.centers.data.uniform_(-s, s)\n",
    "        # size init \n",
    "        self.sigs.data.uniform_(100*s, 100*s)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        # linear part is the same as normal NNs\n",
    "        l = nn.functional.linear(input, self.weights, bias=None)\n",
    "        # optional, apply tanh here\n",
    "        # l = torch.tanh(l)\n",
    "#         print(\"size of L\", l.size())\n",
    "#         print(l)\n",
    "\n",
    "\n",
    "        # gaussian component\n",
    "        # unsqueeze the inputs to allow broadcasting\n",
    "        # compute distance to centers\n",
    "        g = (input.unsqueeze(1)-self.centers)**2\n",
    "#         print(\"size of G\", g.size())\n",
    "        # print(g)\n",
    "        g = g.sum(dim=2)\n",
    "#         print(\"size of G\", g.size())\n",
    "\n",
    "        # for future, use any norm?\n",
    "        # g2 = torch.norm(self.input.unsqueeze(1)-centers), p=2, dim=2)\n",
    "\n",
    "        # apply sigma\n",
    "        g = -g/(self.sigs**2)\n",
    "#         print(\"size of G\", g.size())\n",
    "        # apply exponential\n",
    "        g = torch.exp(g)\n",
    "#         print(\"size of G\", g.size())\n",
    "\n",
    "        # combine gaussian with linear\n",
    "        res = l*g\n",
    "        # optional, flatten res\n",
    "        # res = F.tanh(res)\n",
    "#         print(\"size of L*G\", res.size())\n",
    "\n",
    "        # clip res to +1\n",
    "        res  = torch.clamp(res, min=-1.0, max=1.0)\n",
    "        \n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dev - is the computation correct?\n",
    "# inputs = torch.randint(low=0, high=10, size=(2,3))\n",
    "# print(\"inputs:\\n\", inputs)\n",
    "# weights = torch.randint(low=-1, high=1, size=(4,3))\n",
    "# print(\"weights:\\n\", weights)\n",
    "# centers = torch.randint(low=-1, high=1, size=(4,3))\n",
    "# print(\"centers:\\n\", centers)\n",
    "# sigs = torch.randint(low=-1, high=1, size=(4,))\n",
    "# print(\"sigs:\\n\", sigs)\n",
    "\n",
    "# # linear part is the same as normal NNs\n",
    "# l = nn.functional.linear(inputs, weights, bias=None)\n",
    "# # optional, apply tanh here\n",
    "# # l = torch.tanh(l)\n",
    "# print(\"size of L\", l.size())\n",
    "# print(l)\n",
    "\n",
    "\n",
    "# # gaussian component\n",
    "# # unsqueeze the inputs to allow broadcasting\n",
    "# # compute distance to centers\n",
    "# g = (inputs.unsqueeze(1)-centers)**2\n",
    "# print(\"size of G\", g.size())\n",
    "# # print(g)\n",
    "# g = g.sum(dim=2)\n",
    "\n",
    "# # for future, use any norm?\n",
    "# # g2 = torch.norm(inputs.unsqueeze(1)-centers), p=2, dim=2)\n",
    "\n",
    "\n",
    "# print(\"size of G\", g.size())\n",
    "# # apply sigma\n",
    "# g = -g/(sigs**2)\n",
    "# print(\"size of G\", g.size())\n",
    "# # apply exponential\n",
    "# g = torch.exp(g)\n",
    "# print(\"size of G\", g.size())\n",
    "# # # g = g.view(l.size())\n",
    "# # print(\"size of G\", g.size())\n",
    "# # print(g)\n",
    "\n",
    "# # g = torch.exp(-((input-self.centers)**2).sum(dim=1)/(self.sigs**2))\n",
    "\n",
    "# # combine\n",
    "# res = l*g\n",
    "# # optional, flatten res\n",
    "# # res = F.tanh(res)\n",
    "# #         print(\"size of L*G\", res.size())\n",
    "\n",
    "# # clip res to +1\n",
    "# res  = torch.clamp(res, min=-1.0, max=1.0)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset and dataloader declaration\n",
    "# transforms does both the conversion from 0-255 to 0-1\n",
    "# and normalizes by the precomputed mean and std\n",
    "\n",
    "batchsize = 30000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../MNIST-dataset', train=True, download=False, \n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])), \n",
    "        batch_size=256, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../MNIST-dataset', train=False, download=False, \n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])), \n",
    "        batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Finite Gaussian Neural Network\n",
    "\n",
    "class FGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FGNet, self).__init__()\n",
    "        self.l1 = FGN_layer(28*28,100)\n",
    "        self.l2 = FGN_layer(100,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the image first\n",
    "        x = x.view(-1, 28*28)\n",
    "#         print(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "#         print(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "        \n",
    "#         final_r = []\n",
    "\n",
    "#         for sample in x:\n",
    "#             r = sample.view(-1,28*28)\n",
    "#             r = self.l1(r)\n",
    "#             r = self.l2(r)\n",
    "\n",
    "#             final_r.append(r)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \n",
    "    losses = th.AverageMeter()\n",
    "    \n",
    "    # set model to trainable mode\n",
    "    model.train()\n",
    "    # load a batch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # load batch data, targets to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # reset optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute predictions\n",
    "        output = model(data)\n",
    "        # compute loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # print(loss)\n",
    "        # update rolling average\n",
    "        losses.update(loss.item(), data.size(0) )\n",
    "        # propagate gradients\n",
    "        loss.backward()\n",
    "#         print(\"max grad centers:\", torch.max(model.l1.centers.grad) )\n",
    "#         print(\"max grad sigs:\", torch.max(model.l1.sigs.grad) )\n",
    "\n",
    "        # apply stored gradients to parameters\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10000 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "            print(\"Loss\", losses.avg)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False ).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Loss 2.58909392357\n",
      "\n",
      "Test set: Average loss: 2.6373, Accuracy: 1110/10000 (11%)\n",
      "\n",
      "Loss 2.69168257713\n",
      "\n",
      "Test set: Average loss: 2.6358, Accuracy: 1109/10000 (11%)\n",
      "\n",
      "Loss 2.6107776165\n",
      "\n",
      "Test set: Average loss: 2.6345, Accuracy: 1109/10000 (11%)\n",
      "\n",
      "Loss 2.51916527748\n",
      "\n",
      "Test set: Average loss: 2.6333, Accuracy: 1112/10000 (11%)\n",
      "\n",
      "Loss 2.65301418304\n",
      "\n",
      "Test set: Average loss: 2.6321, Accuracy: 1113/10000 (11%)\n",
      "\n",
      "Loss 2.56910300255\n",
      "\n",
      "Test set: Average loss: 2.6309, Accuracy: 1115/10000 (11%)\n",
      "\n",
      "Loss 2.68103909492\n",
      "\n",
      "Test set: Average loss: 2.6295, Accuracy: 1117/10000 (11%)\n",
      "\n",
      "Loss 2.63765311241\n",
      "\n",
      "Test set: Average loss: 2.6283, Accuracy: 1118/10000 (11%)\n",
      "\n",
      "Loss 2.66585612297\n",
      "\n",
      "Test set: Average loss: 2.6272, Accuracy: 1119/10000 (11%)\n",
      "\n",
      "Loss 2.66963744164\n",
      "\n",
      "Test set: Average loss: 2.6261, Accuracy: 1119/10000 (11%)\n",
      "\n",
      "Loss 2.58609890938\n",
      "\n",
      "Test set: Average loss: 2.6250, Accuracy: 1119/10000 (11%)\n",
      "\n",
      "Loss 2.55832338333\n",
      "\n",
      "Test set: Average loss: 2.6238, Accuracy: 1120/10000 (11%)\n",
      "\n",
      "Loss 2.62238049507\n",
      "\n",
      "Test set: Average loss: 2.6225, Accuracy: 1122/10000 (11%)\n",
      "\n",
      "Loss 2.56276631355\n",
      "\n",
      "Test set: Average loss: 2.6215, Accuracy: 1120/10000 (11%)\n",
      "\n",
      "Loss 2.65930128098\n",
      "\n",
      "Test set: Average loss: 2.6202, Accuracy: 1120/10000 (11%)\n",
      "\n",
      "Loss 2.62803411484\n",
      "\n",
      "Test set: Average loss: 2.6191, Accuracy: 1120/10000 (11%)\n",
      "\n",
      "Loss 2.63930678368\n",
      "\n",
      "Test set: Average loss: 2.6180, Accuracy: 1123/10000 (11%)\n",
      "\n",
      "Loss 2.53407430649\n",
      "\n",
      "Test set: Average loss: 2.6170, Accuracy: 1126/10000 (11%)\n",
      "\n",
      "Loss 2.60800266266\n",
      "\n",
      "Test set: Average loss: 2.6160, Accuracy: 1128/10000 (11%)\n",
      "\n",
      "Loss 2.68991708755\n",
      "\n",
      "Test set: Average loss: 2.6150, Accuracy: 1130/10000 (11%)\n",
      "\n",
      "Loss 2.64317178726\n",
      "\n",
      "Test set: Average loss: 2.6141, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Loss 2.64600658417\n",
      "\n",
      "Test set: Average loss: 2.6131, Accuracy: 1134/10000 (11%)\n",
      "\n",
      "Loss 2.60052227974\n",
      "\n",
      "Test set: Average loss: 2.6121, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Loss 2.65402770042\n",
      "\n",
      "Test set: Average loss: 2.6110, Accuracy: 1136/10000 (11%)\n",
      "\n",
      "Loss 2.552713871\n",
      "\n",
      "Test set: Average loss: 2.6101, Accuracy: 1136/10000 (11%)\n",
      "\n",
      "Loss 2.59914159775\n",
      "\n",
      "Test set: Average loss: 2.6089, Accuracy: 1136/10000 (11%)\n",
      "\n",
      "Loss 2.60449743271\n",
      "\n",
      "Test set: Average loss: 2.6079, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "Loss 2.61061620712\n",
      "\n",
      "Test set: Average loss: 2.6068, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "Loss 2.55129861832\n",
      "\n",
      "Test set: Average loss: 2.6057, Accuracy: 1138/10000 (11%)\n",
      "\n",
      "Loss 2.62265634537\n",
      "\n",
      "Test set: Average loss: 2.6045, Accuracy: 1138/10000 (11%)\n",
      "\n",
      "Loss 2.61777257919\n",
      "\n",
      "Test set: Average loss: 2.6035, Accuracy: 1139/10000 (11%)\n",
      "\n",
      "Loss 2.55114197731\n",
      "\n",
      "Test set: Average loss: 2.6023, Accuracy: 1140/10000 (11%)\n",
      "\n",
      "Loss 2.61655330658\n",
      "\n",
      "Test set: Average loss: 2.6013, Accuracy: 1142/10000 (11%)\n",
      "\n",
      "Loss 2.50022101402\n",
      "\n",
      "Test set: Average loss: 2.6004, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Loss 2.72745084763\n",
      "\n",
      "Test set: Average loss: 2.5994, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Loss 2.68804192543\n",
      "\n",
      "Test set: Average loss: 2.5984, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Loss 2.65986251831\n",
      "\n",
      "Test set: Average loss: 2.5973, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Loss 2.56905698776\n",
      "\n",
      "Test set: Average loss: 2.5962, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Loss 2.58665895462\n",
      "\n",
      "Test set: Average loss: 2.5951, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Loss 2.7105448246\n",
      "\n",
      "Test set: Average loss: 2.5939, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Loss 2.58124375343\n",
      "\n",
      "Test set: Average loss: 2.5923, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Loss 2.60338521004\n",
      "\n",
      "Test set: Average loss: 2.5912, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Loss 2.62042093277\n",
      "\n",
      "Test set: Average loss: 2.5903, Accuracy: 1149/10000 (11%)\n",
      "\n",
      "Loss 2.56307005882\n",
      "\n",
      "Test set: Average loss: 2.5893, Accuracy: 1151/10000 (12%)\n",
      "\n",
      "Loss 2.6962594986\n",
      "\n",
      "Test set: Average loss: 2.5881, Accuracy: 1153/10000 (12%)\n",
      "\n",
      "Loss 2.57057309151\n",
      "\n",
      "Test set: Average loss: 2.5871, Accuracy: 1154/10000 (12%)\n",
      "\n",
      "Loss 2.61592793465\n",
      "\n",
      "Test set: Average loss: 2.5861, Accuracy: 1155/10000 (12%)\n",
      "\n",
      "Loss 2.57919430733\n",
      "\n",
      "Test set: Average loss: 2.5852, Accuracy: 1157/10000 (12%)\n",
      "\n",
      "Loss 2.47188234329\n",
      "\n",
      "Test set: Average loss: 2.5842, Accuracy: 1158/10000 (12%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "torch.manual_seed(666)\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize the network\n",
    "model = FGNet().to(device)\n",
    "\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01, momentum=0.5)\n",
    "\n",
    "# train the model \n",
    "for epoch in range(1, 50):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
