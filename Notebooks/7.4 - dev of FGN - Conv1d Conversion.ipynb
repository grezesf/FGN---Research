{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a7eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev of code to convert a Conv1d layer to a FGN_Conv1D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97f1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c39151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/felix/Research/Adversarial Research/FGN---Research/')\n",
    "import Finite_Gaussian_Network_lib as fgnl\n",
    "import Finite_Gaussian_Network_lib.fgn_helper_lib as fgnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0f9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define layers\n",
    "in_channels = 2\n",
    "out_channels = 3\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "\n",
    "conv1d =  nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "fgn_conv1d = fgnl.FGN_Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecae9279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[ 0.3778,  0.4663],\n",
       "                       [ 0.0803,  0.0208]],\n",
       "              \n",
       "                      [[ 0.3700,  0.0154],\n",
       "                       [ 0.0856,  0.4929]],\n",
       "              \n",
       "                      [[-0.1171,  0.2407],\n",
       "                       [ 0.1383, -0.3520]]])),\n",
       "             ('bias', tensor([ 0.4130,  0.3831, -0.3741]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1cf41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('centers',\n",
       "              tensor([[[ 0.1003, -0.0468],\n",
       "                       [ 0.0339, -0.0394],\n",
       "                       [ 0.1164, -0.0606]],\n",
       "              \n",
       "                      [[ 0.2659,  0.0077],\n",
       "                       [-0.1787,  0.0214],\n",
       "                       [ 0.0593, -0.0697]]])),\n",
       "             ('inv_covars',\n",
       "              tensor([[1.7493, 1.8115],\n",
       "                      [1.2066, 1.5190],\n",
       "                      [1.2027, 1.7948]])),\n",
       "             ('Conv1d.weight',\n",
       "              tensor([[[ 0.3472, -0.1723],\n",
       "                       [ 0.0490,  0.3529]],\n",
       "              \n",
       "                      [[ 0.4220,  0.3231],\n",
       "                       [-0.1072,  0.1632]],\n",
       "              \n",
       "                      [[ 0.4824, -0.2758],\n",
       "                       [ 0.4238, -0.3601]]])),\n",
       "             ('Conv1d.bias', tensor([-0.1370,  0.2160, -0.0739]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fgn_conv1d.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36a8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert\n",
    "fgn_conv1d.Conv1d.weight = conv1d.weight\n",
    "fgn_conv1d.Conv1d.bias = conv1d.bias\n",
    "fgn_conv1d.centers = torch.nn.Parameter( \n",
    "    torch.stack([(-b/torch.dot(w.flatten(),w.flatten()))*w.flatten()\n",
    "                 for (b,w) in zip(conv1d.bias,\n",
    "                                  conv1d.weight)]).reshape(in_channels, out_channels, kernel_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49aff0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem is new_centers has wrong shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec4c158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4251, -0.5247],\n",
       "         [-0.0903, -0.0234],\n",
       "         [-0.3659, -0.0152]],\n",
       "\n",
       "        [[-0.0846, -0.4875],\n",
       "         [-0.2041,  0.4194],\n",
       "         [ 0.2410, -0.6133]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([(-b/torch.dot(w.flatten(),w.flatten()))*w.flatten()\n",
    "                 for (b,w) in zip(conv1d.bias,\n",
    "                                  conv1d.weight)]).reshape(in_channels, out_channels, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e5da49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[ 0.3778,  0.4663],\n",
       "                       [ 0.0803,  0.0208]],\n",
       "              \n",
       "                      [[ 0.3700,  0.0154],\n",
       "                       [ 0.0856,  0.4929]],\n",
       "              \n",
       "                      [[-0.1171,  0.2407],\n",
       "                       [ 0.1383, -0.3520]]])),\n",
       "             ('bias', tensor([ 0.4130,  0.3831, -0.3741]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee34fea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('centers',\n",
       "              tensor([[[-0.4251, -0.5247],\n",
       "                       [-0.0903, -0.0234],\n",
       "                       [-0.3659, -0.0152]],\n",
       "              \n",
       "                      [[-0.0846, -0.4875],\n",
       "                       [-0.2041,  0.4194],\n",
       "                       [ 0.2410, -0.6133]]])),\n",
       "             ('inv_covars',\n",
       "              tensor([[1.7493, 1.8115],\n",
       "                      [1.2066, 1.5190],\n",
       "                      [1.2027, 1.7948]])),\n",
       "             ('Conv1d.weight',\n",
       "              tensor([[[ 0.3778,  0.4663],\n",
       "                       [ 0.0803,  0.0208]],\n",
       "              \n",
       "                      [[ 0.3700,  0.0154],\n",
       "                       [ 0.0856,  0.4929]],\n",
       "              \n",
       "                      [[-0.1171,  0.2407],\n",
       "                       [ 0.1383, -0.3520]]])),\n",
       "             ('Conv1d.bias', tensor([ 0.4130,  0.3831, -0.3741]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fgn_conv1d.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b11f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c74713cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3778, 0.4663, 0.0803, 0.0208], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = conv1d.weight[0].flatten()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e46090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4130, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = conv1d.bias[0]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5593541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4251, -0.5247, -0.0903, -0.0234], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-b/torch.dot(w,w))*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df9db26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 2]), torch.Size([3]), torch.Size([2, 3, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conv1d.weight.shape, conv1d.bias.shape, fgn_conv1d.centers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75bdeedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4251, -0.5247, -0.0903, -0.0234],\n",
      "        [-0.3659, -0.0152, -0.0846, -0.4875],\n",
      "        [-0.2041,  0.4194,  0.2410, -0.6133]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "new_centers = torch.stack([(-b/torch.dot(w.flatten(),w.flatten()))*w.flatten()\n",
    "               for (b,w) in zip(conv1d.bias, \n",
    "                                conv1d.weight)])\n",
    "print(new_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d5940eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2, 8000])\n"
     ]
    }
   ],
   "source": [
    "# test same behavior\n",
    "inputs = torch.rand((100,in_channels,8000))\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8ee09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase inv_covar size\n",
    "fgn_conv1d.inv_covars = torch.nn.Parameter(fgn_conv1d.inv_covars/25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b5766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[6.9972e-05, 7.2461e-05],\n",
       "        [4.8263e-05, 6.0758e-05],\n",
       "        [4.8108e-05, 7.1791e-05]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fgn_conv1d.inv_covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f941af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 7999])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = conv1d(inputs)\n",
    "r1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7158cf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 7999])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = fgn_conv1d(inputs)[0]\n",
    "r2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a2648a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0017, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(abs(r2-r1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce10d69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all((r2==r1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb9d7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define converter function\n",
    "def convert_layer_conv1D_to_fgn(classic_layer, fgn_layer,  init_factor=25000):\n",
    "    # changes the weights of the fgn_layer to match the behavior of the classic_layer\n",
    "    # the two layers MUST have been created with the same parameters\n",
    "    \n",
    "    # some size checks\n",
    "    assert(fgn_layer.Conv1d.weight.shape==classic_layer.weight.shape)\n",
    "    assert(fgn_layer.Conv1d.bias.shape==classic_layer.bias.shape)\n",
    "\n",
    "    # convert params\n",
    "    fgn_layer.Conv1d.weight = classic_layer.weight\n",
    "    fgn_layer.Conv1d.bias = classic_layer.bias\n",
    "    fgn_layer.centers = torch.nn.Parameter(torch.stack([(-b/torch.dot(w.flatten(),w.flatten()))*w.flatten()\n",
    "                                                        for (b,w) in zip(classic_layer.bias,\n",
    "                                                                         classic_layer.weight)]).reshape(fgn_layer.centers.shape)\n",
    "                                          )\n",
    "    \n",
    "    # ensure large sigma, enough to mimic behavior of classic_layer\n",
    "    fgn_layer.inv_covars = torch.nn.Parameter(fgn_layer.inv_covars/init_factor)\n",
    "    \n",
    "    # returns nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d02bd6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define layers\n",
    "in_channels = 3\n",
    "out_channels = 4\n",
    "kernel_size = 5\n",
    "stride = 6\n",
    "\n",
    "conv1d =  nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "fgn_conv1d = fgnl.FGN_Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "inputs = torch.rand((50,in_channels,4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15cbf077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[-2.0033e-01, -9.5478e-02,  2.5541e-01, -7.5028e-02, -2.1559e-01],\n",
       "                       [-1.8583e-01,  5.1569e-02,  2.5164e-01,  1.0181e-01, -2.5646e-01],\n",
       "                       [-1.5950e-01, -1.7417e-01,  2.0022e-01,  1.9771e-01, -1.6366e-01]],\n",
       "              \n",
       "                      [[ 2.4948e-01,  2.2273e-01, -2.2336e-01,  1.0936e-02, -2.0587e-01],\n",
       "                       [-8.7873e-02,  1.2429e-04, -1.5785e-01,  2.4988e-01,  8.1369e-02],\n",
       "                       [ 1.7202e-01, -2.1298e-02, -3.6783e-02, -1.5823e-01, -9.1511e-02]],\n",
       "              \n",
       "                      [[ 2.1292e-01,  1.7634e-01, -8.0644e-02, -1.4680e-01,  2.3455e-02],\n",
       "                       [ 1.2856e-01,  2.2521e-01,  1.9661e-01, -8.9145e-02, -1.0592e-01],\n",
       "                       [-4.1008e-02, -1.7740e-01, -9.3840e-02,  1.9926e-01, -5.8858e-02]],\n",
       "              \n",
       "                      [[-9.1639e-02, -1.9358e-01,  4.3106e-02,  4.4430e-02, -4.8923e-03],\n",
       "                       [-1.8576e-01, -2.4578e-01, -1.9010e-01, -1.5066e-01,  1.4036e-01],\n",
       "                       [ 2.2544e-01,  6.6881e-02, -1.1044e-01,  2.0464e-01,  6.7583e-02]]])),\n",
       "             ('bias', tensor([ 0.1723,  0.0643,  0.1291, -0.0444]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1184d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('centers',\n",
       "              tensor([[[ 0.2271,  0.0702,  0.0757, -0.1630, -0.0433],\n",
       "                       [ 0.0295, -0.2036,  0.1519,  0.0337,  0.2011],\n",
       "                       [-0.0542, -0.0904, -0.0628, -0.0253, -0.0504],\n",
       "                       [ 0.1269,  0.0906,  0.0871, -0.0823,  0.0221]],\n",
       "              \n",
       "                      [[-0.0619, -0.0090,  0.0944,  0.0555,  0.0778],\n",
       "                       [ 0.0320, -0.1525,  0.0481, -0.1035,  0.0676],\n",
       "                       [-0.0683, -0.0337,  0.0485,  0.0871,  0.3085],\n",
       "                       [-0.0954, -0.1151,  0.0520,  0.0801,  0.1007]],\n",
       "              \n",
       "                      [[-0.1525, -0.0468, -0.0406, -0.0391,  0.0298],\n",
       "                       [ 0.0204, -0.0787,  0.0760,  0.1502,  0.0162],\n",
       "                       [ 0.0154,  0.0226,  0.0367,  0.0433, -0.1870],\n",
       "                       [ 0.0861,  0.0564, -0.0315,  0.1019, -0.0572]]])),\n",
       "             ('inv_covars',\n",
       "              tensor([[0.5504, 0.6020, 0.6304, 0.6030, 0.6366],\n",
       "                      [0.5819, 0.5678, 0.5562, 0.5907, 0.6516],\n",
       "                      [0.6609, 0.5864, 0.5630, 0.6088, 0.5939],\n",
       "                      [0.6452, 0.6301, 0.6593, 0.5531, 0.5884]])),\n",
       "             ('Conv1d.weight',\n",
       "              tensor([[[-0.0109,  0.2029,  0.0337, -0.1242, -0.1813],\n",
       "                       [-0.1106, -0.1779,  0.2319, -0.1051, -0.1601],\n",
       "                       [ 0.0956,  0.2549,  0.0500,  0.0460, -0.0175]],\n",
       "              \n",
       "                      [[-0.0502, -0.0983,  0.0537, -0.0839, -0.0841],\n",
       "                       [-0.2394,  0.0458, -0.0890, -0.2141, -0.0513],\n",
       "                       [ 0.1368, -0.1704,  0.2351, -0.0404,  0.0804]],\n",
       "              \n",
       "                      [[ 0.2305,  0.2279,  0.1895, -0.1609,  0.0417],\n",
       "                       [-0.0664,  0.0084, -0.2067,  0.0612,  0.1923],\n",
       "                       [-0.1648,  0.0781,  0.0735, -0.1533, -0.1823]],\n",
       "              \n",
       "                      [[ 0.1117,  0.2516,  0.2511, -0.0521,  0.2501],\n",
       "                       [-0.2509,  0.0987,  0.0435,  0.1286,  0.2324],\n",
       "                       [ 0.0599,  0.1420,  0.1410,  0.0434,  0.1930]]])),\n",
       "             ('Conv1d.bias', tensor([ 0.0856, -0.0979,  0.1563, -0.0482]))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fgn_conv1d.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bedbc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_layer_conv1D_to_fgn(conv1d, fgn_conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66a8b2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('centers',\n",
       "              tensor([[[ 6.8346e-02,  3.2573e-02, -8.7135e-02,  2.5597e-02,  7.3551e-02],\n",
       "                       [ 6.3398e-02, -1.7593e-02, -8.5850e-02, -3.4735e-02,  8.7493e-02],\n",
       "                       [ 5.4416e-02,  5.9419e-02, -6.8306e-02, -6.7450e-02,  5.5836e-02],\n",
       "                       [-4.3287e-02, -3.8647e-02,  3.8754e-02, -1.8974e-03,  3.5721e-02]],\n",
       "              \n",
       "                      [[ 1.5247e-02, -2.1565e-05,  2.7389e-02, -4.3356e-02, -1.4118e-02],\n",
       "                       [-2.9847e-02,  3.6955e-03,  6.3822e-03,  2.7454e-02,  1.5878e-02],\n",
       "                       [-8.7180e-02, -7.2203e-02,  3.3020e-02,  6.0108e-02, -9.6039e-03],\n",
       "                       [-5.2638e-02, -9.2215e-02, -8.0503e-02,  3.6501e-02,  4.3371e-02]],\n",
       "              \n",
       "                      [[ 1.6791e-02,  7.2639e-02,  3.8423e-02, -8.1588e-02,  2.4100e-02],\n",
       "                       [-1.2061e-02, -2.5478e-02,  5.6735e-03,  5.8478e-03, -6.4392e-04],\n",
       "                       [-2.4450e-02, -3.2349e-02, -2.5021e-02, -1.9829e-02,  1.8474e-02],\n",
       "                       [ 2.9672e-02,  8.8027e-03, -1.4535e-02,  2.6935e-02,  8.8951e-03]]])),\n",
       "             ('inv_covars',\n",
       "              tensor([[2.2016e-05, 2.4079e-05, 2.5215e-05, 2.4122e-05, 2.5464e-05],\n",
       "                      [2.3277e-05, 2.2712e-05, 2.2249e-05, 2.3628e-05, 2.6065e-05],\n",
       "                      [2.6436e-05, 2.3455e-05, 2.2518e-05, 2.4350e-05, 2.3756e-05],\n",
       "                      [2.5808e-05, 2.5203e-05, 2.6373e-05, 2.2123e-05, 2.3538e-05]])),\n",
       "             ('Conv1d.weight',\n",
       "              tensor([[[-2.0033e-01, -9.5478e-02,  2.5541e-01, -7.5028e-02, -2.1559e-01],\n",
       "                       [-1.8583e-01,  5.1569e-02,  2.5164e-01,  1.0181e-01, -2.5646e-01],\n",
       "                       [-1.5950e-01, -1.7417e-01,  2.0022e-01,  1.9771e-01, -1.6366e-01]],\n",
       "              \n",
       "                      [[ 2.4948e-01,  2.2273e-01, -2.2336e-01,  1.0936e-02, -2.0587e-01],\n",
       "                       [-8.7873e-02,  1.2429e-04, -1.5785e-01,  2.4988e-01,  8.1369e-02],\n",
       "                       [ 1.7202e-01, -2.1298e-02, -3.6783e-02, -1.5823e-01, -9.1511e-02]],\n",
       "              \n",
       "                      [[ 2.1292e-01,  1.7634e-01, -8.0644e-02, -1.4680e-01,  2.3455e-02],\n",
       "                       [ 1.2856e-01,  2.2521e-01,  1.9661e-01, -8.9145e-02, -1.0592e-01],\n",
       "                       [-4.1008e-02, -1.7740e-01, -9.3840e-02,  1.9926e-01, -5.8858e-02]],\n",
       "              \n",
       "                      [[-9.1639e-02, -1.9358e-01,  4.3106e-02,  4.4430e-02, -4.8923e-03],\n",
       "                       [-1.8576e-01, -2.4578e-01, -1.9010e-01, -1.5066e-01,  1.4036e-01],\n",
       "                       [ 2.2544e-01,  6.6881e-02, -1.1044e-01,  2.0464e-01,  6.7583e-02]]])),\n",
       "             ('Conv1d.bias', tensor([ 0.1723,  0.0643,  0.1291, -0.0444]))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fgn_conv1d.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2cdecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 4, 666])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = conv1d(inputs)\n",
    "r1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2e6b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 4, 666])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = fgn_conv1d(inputs)[0]\n",
    "r2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "783e271d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(abs(r2-r1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cd535c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all((r2==r1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf42b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
