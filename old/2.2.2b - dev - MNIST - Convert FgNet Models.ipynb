{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "# 1 train an classical model\n",
    "# 2 convert to fgn\n",
    "# 3 retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "# import sys\n",
    "# sys.path.append('/home/felix/Research/Adversarial Research/FGN---Research')\n",
    "import torch_helper_lib as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# manualy set cuda device\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random seeds\n",
    "# torch.manual_seed(999)\n",
    "# np.random.seed(999)\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.cuda.manual_seed_all(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset and dataloader declaration\n",
    "# transforms does both the conversion from 0-255 to 0-1\n",
    "# and normalizes by the precomputed mean and std\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "mnist_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../MNIST-dataset', train=True, download=False, \n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])), \n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mnist_test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../MNIST-dataset', train=False, download=False, \n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])), \n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.66666666667e-06\n"
     ]
    }
   ],
   "source": [
    "# loss functions for the classic net\n",
    "lmbda_l2 = (4.0*0.1/len(mnist_train_loader.dataset))\n",
    "print(lmbda_l2)\n",
    "      \n",
    "def l2_loss(model):\n",
    "    # sum of w^2 for regularizer \n",
    "    for p in model.named_parameters():\n",
    "        if ('weight' in p[0]) or ('bias' in p[0]):\n",
    "            try:\n",
    "                l2 += torch.sum(p[1]**2)\n",
    "            except:\n",
    "                l2 = torch.sum(p[1]**2)          \n",
    "    return l2\n",
    "\n",
    "\n",
    "def classical_cross_ent_loss(model, output, target):\n",
    "    cent_loss = F.cross_entropy(output, target.long())\n",
    "    l2 = l2_loss(model)\n",
    "    return cent_loss + lmbda_l2*l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network params\n",
    "hidden_l_nums = [32,32]\n",
    "drop_p= 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classical network\n",
    "classic_model = th.Feedforward_Classic_net(in_feats=28*28, out_feats=10, hidden_l_nums=hidden_l_nums, drop_p=drop_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network optimizer\n",
    "classic_optimizer = optim.RMSprop(filter(lambda p: p.requires_grad, classic_model.parameters()),momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train set - Average loss: 0.6503, Accuracy: 47631/60000 (79%)\n",
      "Test set - Average loss: 0.4206, Accuracy: 8805/10000 (88%)\n",
      "Epoch 1 Train set - Average loss: 0.3520, Accuracy: 53915/60000 (90%)\n",
      "Test set - Average loss: 0.3659, Accuracy: 8950/10000 (90%)\n",
      "Epoch 2 Train set - Average loss: 0.2987, Accuracy: 54841/60000 (91%)\n",
      "Test set - Average loss: 0.3191, Accuracy: 9108/10000 (91%)\n",
      "Epoch 3 Train set - Average loss: 0.2736, Accuracy: 55378/60000 (92%)\n",
      "Test set - Average loss: 0.3387, Accuracy: 9059/10000 (91%)\n",
      "Epoch 4 Train set - Average loss: 0.2576, Accuracy: 55616/60000 (93%)\n",
      "Test set - Average loss: 0.2795, Accuracy: 9243/10000 (92%)\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "classic_train_res = th.train(classic_model, device, mnist_train_loader, \n",
    "                             classical_cross_ent_loss, classic_optimizer, epochs, save_hist=2, verbose=True, \n",
    "                             pred_func=th.cross_ent_pred_accuracy, test_loader=mnist_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert to FGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model to be converted\n",
    "fgn_model = th.Feedforward_FGN_net(in_feats=28*28, out_feats=10, hidden_l_nums=hidden_l_nums, drop_p=drop_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fl Linear(in_features=32, out_features=10, bias=True)\n",
      "hidden_layers ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "id Dropout(p=0.0)\n"
     ]
    }
   ],
   "source": [
    "for name, module in reversed(classic_model._modules.items()):\n",
    "    print(name, module)\n",
    "    if len(list(module.children())) > 0:\n",
    "        # recurse\n",
    "        classic_model._modules[name = convert_layers(model=module, num_to_convert=num_to_convert-conversion_count, layer_type_old, layer_type_new, convert_weights)\n",
    "        conversion_count += num_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_layers(model, layer_type_old, layer_type_new, convert_weights=False):\n",
    "    \n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        print(name, type(module))\n",
    "        if len(list(module.children())) > 0:\n",
    "            # recurse\n",
    "            print(\"recursing\")\n",
    "            model._modules[name] = convert_layers(module, layer_type_old, layer_type_new, convert_weights)\n",
    "\n",
    "        if type(module) == layer_type_old :\n",
    "            print(\"changing type\")\n",
    "            layer_old = module\n",
    "            layer_new = layer_type_new(28*28,10) \n",
    "\n",
    "            if convert_weights == True:\n",
    "                layer_new.weight = layer_old.weight\n",
    "                layer_new.bias = layer_old.bias\n",
    "\n",
    "            model._modules[name] = layer_new\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fl <class 'torch.nn.modules.linear.Linear'>\n",
      "changing type\n",
      "hidden_layers <class 'torch.nn.modules.container.ModuleList'>\n",
      "recursing\n",
      "3 <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "2 <class 'torch.nn.modules.linear.Linear'>\n",
      "changing type\n",
      "1 <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "0 <class 'torch.nn.modules.linear.Linear'>\n",
      "changing type\n",
      "id <class 'torch.nn.modules.dropout.Dropout'>\n"
     ]
    }
   ],
   "source": [
    "converted_model = convert_layers(classic_model, nn.Linear, th.FGN_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward_Classic_net(\n",
      "  (id): Dropout(p=0.0)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): FGN_layer()\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): FGN_layer()\n",
      "    (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fl): FGN_layer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(converted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Feedforward_FGN_net:\n\tMissing key(s) in state_dict: \"hidden_layers.1.running_var\", \"hidden_layers.1.bias\", \"hidden_layers.1.weight\", \"hidden_layers.1.running_mean\", \"hidden_layers.3.running_var\", \"hidden_layers.3.bias\", \"hidden_layers.3.weight\", \"hidden_layers.3.running_mean\". \n\tsize mismatch for hidden_layers.2.weights: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for hidden_layers.2.centers: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for fl.weights: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([10, 32]).\n\tsize mismatch for fl.centers: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([10, 32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-53a7a18a7aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_Classic2FGN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassic_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgn_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfgn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model converted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/felix/Research/Adversarial Research/FGN---Research/torch_helper_lib/convert_Classic2FGN.pyc\u001b[0m in \u001b[0;36mconvert_Classic2FGN\u001b[0;34m(classic_model, fgn_model)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mnew_state_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_state_dict_lin2FGN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mfgn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# return nothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Feedforward_FGN_net:\n\tMissing key(s) in state_dict: \"hidden_layers.1.running_var\", \"hidden_layers.1.bias\", \"hidden_layers.1.weight\", \"hidden_layers.1.running_mean\", \"hidden_layers.3.running_var\", \"hidden_layers.3.bias\", \"hidden_layers.3.weight\", \"hidden_layers.3.running_mean\". \n\tsize mismatch for hidden_layers.2.weights: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for hidden_layers.2.centers: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for fl.weights: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([10, 32]).\n\tsize mismatch for fl.centers: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([10, 32])."
     ]
    }
   ],
   "source": [
    "# convert\n",
    "th.convert_Classic2FGN(classic_model=classic_model, fgn_model=fgn_model)\n",
    "print(\"Model converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
